{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A simple Text Classifier #\n",
    "Author: Christin Seifert, licensed under the Creative Commons Attribution 3.0 Unported License https://creativecommons.org/licenses/by/3.0/ \n",
    "It is based on a tutorial of Nils Witt (https://github.com/n-witt/MachineLearningWithText_SS2017)\n",
    "\n",
    "\n",
    "This is a tutorial for learning and evaluating a simple naive bayes classifier on for a simple text classification problem. In this tutorial you will:\n",
    "\n",
    "* inspect the data you will be using to train the decision tree \n",
    "* train a decision tree \n",
    "* evaluate how well the decision tree does \n",
    "* visualize the decision tree\n",
    "\n",
    "It is assumed that you have some general knowledge on \n",
    "* document-term matrices\n",
    "* what a Naive Bayes classifier does"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting texts to features\n",
    "\n",
    "We wil start with a small example of 3 SMS'. The texts in the SMS are the following \"call me tonight\", \"Call me a cab\", \"please call me... PLEASE!\" In order to do text classification we need to convert the text into a feature vector. We will follow a very simple approach here:\n",
    "1. Find out which different words (or tokens) are used in the text. These makes up the vocabulary.\n",
    "2. The length of a vector for each document then is the size of the vocabulary, and each entry in the vector corresponds to one word. This means, the first entry in the vector corresponds to the first word in the vocabulary, the second to the second and .. you get the logic ;-)\n",
    "3. For each document we simply cound how often each word occurs and write it at the index in the vector that corresponds to this word. \n",
    "\n",
    "All those things can easily be done with the [CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) from the sklearn library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_train = ['call you tonight', 'Call me a cab', 'please call me... PLEASE!']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import and instantiate CountVectorizer (with the default parameters)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# learn the 'vocabulary' of the training data \n",
    "vect.fit(simple_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cab', 'call', 'me', 'please', 'tonight', 'you']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examine the fitted vocabulary\n",
    "vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have you noticed that all words are lower case now? And that we ignored punctuation? Whether this is a good idea, depends on the application. E.g. for detecting emotions in texts, smilies (punctutation) might be a helpful feature. But for now, let's keep it simple.\n",
    "\n",
    "Now we generate a document-term matrix. In this matrix each row corresponds to one document, each column to one feature. Entry `(i,j)` tells us how often word `j` occurs in document `i`.\n",
    "\n",
    "_Note:_ The \"how often\" is only true if we use the count vectorizer. Instead of word count there are many other possible features.\n",
    "\n",
    "From the [scikit-learn documentation](http://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction):\n",
    "\n",
    "> In this scheme, features and samples are defined as follows:\n",
    "\n",
    "> - Each individual token occurrence frequency (normalized or not) is treated as a **feature**.\n",
    "> - The vector of all the token frequencies for a given document is considered a **sample**.\n",
    "\n",
    "> A **corpus of documents** can thus be represented by a matrix with **one row per document** and **one column per token** (e.g. word) occurring in the corpus.\n",
    "\n",
    "> We call **vectorization** the general process of turning a collection of text documents into numerical feature vectors. This specific strategy (tokenization, counting and normalization) is called the **Bag of Words** or \"Bag of n-grams\" representation. Documents are described by word occurrences while completely ignoring the relative position information of the words in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x6 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 9 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transform training data into a 'document-term matrix'\n",
    "simple_train_dtm = vect.transform(simple_train)\n",
    "simple_train_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 0, 1, 1],\n",
       "       [1, 1, 1, 0, 0, 0],\n",
       "       [0, 1, 1, 2, 0, 0]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert sparse matrix to a dense matrix\n",
    "simple_train_dtm.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use a pandas data frame to store the vector and the feature names together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cab</th>\n",
       "      <th>call</th>\n",
       "      <th>me</th>\n",
       "      <th>please</th>\n",
       "      <th>tonight</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cab  call  me  please  tonight  you\n",
       "0    0     1   0       0        1    1\n",
       "1    1     1   1       0        0    0\n",
       "2    0     1   1       2        0    0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examine the vocabulary and document-term matrix together\n",
    "import pandas as pd\n",
    "pd.DataFrame(simple_train_dtm.toarray(), columns=vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since in general this is an aweful lot of zeros (think of how many of all English words are present in a SMS), the more efficient way to store the information is as a sparse matrix. For humans this is a bit harder to read.\n",
    "\n",
    "From the [scikit-learn documentation](http://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction):\n",
    "\n",
    "> As most documents will typically use a very small subset of the words used in the corpus, the resulting matrix will have **many feature values that are zeros** (typically more than 99% of them).\n",
    "\n",
    "> For instance, a collection of 10,000 short text documents (such as emails) will use a vocabulary with a size in the order of 100,000 unique words in total while each document will use 100 to 1000 unique words individually.\n",
    "\n",
    "> In order to be able to **store such a matrix in memory** but also to **speed up operations**, implementations will typically use a **sparse representation** such as the implementations available in the `scipy.sparse` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the type of the document-term matrix\n",
    "type(simple_train_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1)\t1\n",
      "  (0, 4)\t1\n",
      "  (0, 5)\t1\n",
      "  (1, 0)\t1\n",
      "  (1, 1)\t1\n",
      "  (1, 2)\t1\n",
      "  (2, 1)\t1\n",
      "  (2, 2)\t1\n",
      "  (2, 3)\t2\n"
     ]
    }
   ],
   "source": [
    "# examine the sparse matrix contents\n",
    "print(simple_train_dtm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the feature vector for a previously unseen text\n",
    "In order to make predictions for unseen data, the new observation must have the same features as the training observations, both in number and meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example text for model testing\n",
    "simple_test = [\"please don't call me, I don't like you\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 1, 0, 1]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transform testing data into a document-term matrix (using existing vocabulary)\n",
    "simple_test_dtm = vect.transform(simple_test)\n",
    "simple_test_dtm.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cab</th>\n",
       "      <th>call</th>\n",
       "      <th>me</th>\n",
       "      <th>please</th>\n",
       "      <th>tonight</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cab  call  me  please  tonight  you\n",
       "0    0     1   1       1        0    1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examine the vocabulary and document-term matrix together\n",
    "pd.DataFrame(simple_test_dtm.toarray(), columns=vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simple spam filter \n",
    "Now we are going to implement a simple spam filter for SMS messages. We are given a data set with SMS that are already annotated with either spam or ham (=not spam). We first load the data set and have a look at the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'material/sms.tsv'\n",
    "sms = pd.read_table(path, header=None, names=['label', 'message'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5572, 2)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>spam</td>\n",
       "      <td>FreeMsg Hey there darling it's been 3 week's n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ham</td>\n",
       "      <td>As per your request 'Melle Melle (Oru Minnamin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>spam</td>\n",
       "      <td>WINNER!! As a valued network customer you have...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>spam</td>\n",
       "      <td>Had your mobile 11 months or more? U R entitle...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...\n",
       "5  spam  FreeMsg Hey there darling it's been 3 week's n...\n",
       "6   ham  Even my brother is not like to speak with me. ...\n",
       "7   ham  As per your request 'Melle Melle (Oru Minnamin...\n",
       "8  spam  WINNER!! As a valued network customer you have...\n",
       "9  spam  Had your mobile 11 months or more? U R entitle..."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examine the first 10 rows\n",
    "sms.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We convert the label to a numerical value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     4825\n",
       "spam     747\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examine the class distribution\n",
    "sms.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert label to a numerical variable\n",
    "sms['label_num'] = sms.label.map({'ham':0, 'spam':1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "      <th>label_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>spam</td>\n",
       "      <td>FreeMsg Hey there darling it's been 3 week's n...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ham</td>\n",
       "      <td>As per your request 'Melle Melle (Oru Minnamin...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>spam</td>\n",
       "      <td>WINNER!! As a valued network customer you have...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>spam</td>\n",
       "      <td>Had your mobile 11 months or more? U R entitle...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message  label_num\n",
       "0   ham  Go until jurong point, crazy.. Available only ...          0\n",
       "1   ham                      Ok lar... Joking wif u oni...          0\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...          1\n",
       "3   ham  U dun say so early hor... U c already then say...          0\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...          0\n",
       "5  spam  FreeMsg Hey there darling it's been 3 week's n...          1\n",
       "6   ham  Even my brother is not like to speak with me. ...          0\n",
       "7   ham  As per your request 'Melle Melle (Oru Minnamin...          0\n",
       "8  spam  WINNER!! As a valued network customer you have...          1\n",
       "9  spam  Had your mobile 11 months or more? U R entitle...          1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check that the conversion worked\n",
    "sms.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have our text in the column `message` and our label in the column `label_num`. Let's have a look at the sizes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5572,)\n",
      "(5572,)\n"
     ]
    }
   ],
   "source": [
    "# how to define X and y (from the SMS data) for use with COUNTVECTORIZER\n",
    "X = sms.message\n",
    "y = sms.label_num\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And at the text of the first 5 messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Go until jurong point, crazy.. Available only ...\n",
       "1                        Ok lar... Joking wif u oni...\n",
       "2    Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3    U dun say so early hor... U c already then say...\n",
       "4    Nah I don't think he goes to usf, he lives aro...\n",
       "Name: message, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms.message.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now prepare the data for the classifier. First split it into a training and a test set. There is a convenient method `train_test_split` available that helps us with that. We use a fixed random state `random_state=42`to split randomly, but at the same time get the same results each time we run the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4179,)\n",
      "(1393,)\n",
      "(4179,)\n",
      "(1393,)\n"
     ]
    }
   ],
   "source": [
    "# split X and y into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use the data preprocessing knowledge from above and generate the vocabulary. We will do this ONLY on the training data set, because we presume to have no knowledge whatsoever about the test data set. So we don't know the test data's vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn training data vocabulary, then use it to create a document-term matrix\n",
    "vect = CountVectorizer()\n",
    "vect.fit(X_train)\n",
    "X_train_dtm = vect.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4179x7490 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 55879 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examine the document-term matrix\n",
    "X_train_dtm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we transform the test data set using the same vocabulary (that is using the same `vect` object that internally knows the vocabulary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1393x7490 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 16940 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transform testing data (using fitted vocabulary) into a document-term matrix\n",
    "X_test_dtm = vect.transform(X_test)\n",
    "X_test_dtm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building and evaluating a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are at the stage where we have a matrix of features and the corresponding labels. We can now train a classifier for spam detection on sms. We will use [multinomial Naive Bayes](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html):\n",
    "\n",
    "> The multinomial Naive Bayes classifier is suitable for classification with **discrete features** (e.g., word counts for text classification). The multinomial distribution normally requires integer feature counts. However, in practice, fractional counts such as tf-idf may also work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb.fit(X_train_dtm, y_train)\n",
    "y_test_pred = nb.predict(X_test_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9885139985642498"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "metrics.accuracy_score(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1203,    4],\n",
       "       [  12,  174]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the confusion matrix\n",
    "metrics.confusion_matrix(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Spaminess\" of words "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start: the estimator has several fields that allow us to examine its internal state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'winner': 7277,\n",
       " 'as': 1046,\n",
       " 'valued': 7003,\n",
       " 'network': 4582,\n",
       " 'customer': 2051,\n",
       " 'you': 7453,\n",
       " 'have': 3230,\n",
       " 'been': 1244,\n",
       " 'selected': 5803,\n",
       " 'to': 6690,\n",
       " 'receivea': 5446,\n",
       " '900': 698,\n",
       " 'prize': 5243,\n",
       " 'reward': 5586,\n",
       " 'claim': 1757,\n",
       " 'call': 1549,\n",
       " '09061701461': 193,\n",
       " 'code': 1814,\n",
       " 'kl341': 3804,\n",
       " 'valid': 6999,\n",
       " '12': 268,\n",
       " 'hours': 3376,\n",
       " 'only': 4771,\n",
       " 'so': 6079,\n",
       " 'how': 3380,\n",
       " 'scotland': 5756,\n",
       " 'hope': 3352,\n",
       " 'are': 1012,\n",
       " 'not': 4659,\n",
       " 'over': 4851,\n",
       " 'showing': 5934,\n",
       " 'your': 7458,\n",
       " 'jjc': 3682,\n",
       " 'tendencies': 6530,\n",
       " 'take': 6459,\n",
       " 'care': 1596,\n",
       " 'live': 4001,\n",
       " 'the': 6578,\n",
       " 'dream': 2353,\n",
       " 'when': 7227,\n",
       " 'and': 926,\n",
       " 'derek': 2181,\n",
       " 'done': 2318,\n",
       " 'with': 7295,\n",
       " 'class': 1764,\n",
       " 'aight': 850,\n",
       " 'lemme': 3929,\n",
       " 'know': 3811,\n",
       " 'what': 7221,\n",
       " 'up': 6941,\n",
       " 'yo': 7448,\n",
       " 'we': 7169,\n",
       " 'watching': 7155,\n",
       " 'movie': 4446,\n",
       " 'on': 4759,\n",
       " 'netflix': 4579,\n",
       " 'why': 7249,\n",
       " 'don': 2316,\n",
       " 'go': 3044,\n",
       " 'tell': 6519,\n",
       " 'friend': 2889,\n",
       " 're': 5409,\n",
       " 'sure': 6398,\n",
       " 'want': 7130,\n",
       " 'him': 3300,\n",
       " 'because': 1231,\n",
       " 'he': 3239,\n",
       " 'smokes': 6059,\n",
       " 'too': 6730,\n",
       " 'much': 4474,\n",
       " 'then': 6588,\n",
       " 'spend': 6174,\n",
       " 'begging': 1254,\n",
       " 'come': 1841,\n",
       " 'smoke': 6057,\n",
       " 'gosh': 3082,\n",
       " 'that': 6575,\n",
       " 'pain': 4878,\n",
       " 'spose': 6204,\n",
       " 'better': 1282,\n",
       " 'shall': 5866,\n",
       " 'send': 5816,\n",
       " 'exe': 2602,\n",
       " 'mail': 4156,\n",
       " 'id': 3445,\n",
       " 'ok': 4740,\n",
       " 'just': 3728,\n",
       " 'arrived': 1037,\n",
       " 'see': 5791,\n",
       " 'in': 3492,\n",
       " 'couple': 1966,\n",
       " 'days': 2100,\n",
       " 'lt': 4096,\n",
       " 'awake': 1123,\n",
       " 'is': 3600,\n",
       " 'there': 6594,\n",
       " 'snow': 6076,\n",
       " 'sounds': 6137,\n",
       " 'like': 3963,\n",
       " 'plan': 5055,\n",
       " 'cardiff': 1593,\n",
       " 'still': 6270,\n",
       " 'here': 3284,\n",
       " 'cold': 1820,\n",
       " 'sitting': 5995,\n",
       " 'radiator': 5362,\n",
       " 'thanks': 6567,\n",
       " 'for': 2824,\n",
       " 've': 7016,\n",
       " 'lovely': 4077,\n",
       " 'wisheds': 7288,\n",
       " 'rock': 5617,\n",
       " 'no': 4625,\n",
       " 'cancer': 1579,\n",
       " 'moms': 4407,\n",
       " 'making': 4168,\n",
       " 'big': 1298,\n",
       " 'deal': 2108,\n",
       " 'out': 4836,\n",
       " 'of': 4715,\n",
       " 'regular': 5489,\n",
       " 'checkup': 1696,\n",
       " 'aka': 861,\n",
       " 'pap': 4891,\n",
       " 'smear': 6046,\n",
       " 'boltblue': 1369,\n",
       " 'tones': 6720,\n",
       " '150p': 293,\n",
       " 'reply': 5531,\n",
       " 'poly': 5115,\n",
       " 'or': 4802,\n",
       " 'mono': 4416,\n",
       " 'eg': 2451,\n",
       " 'poly3': 5116,\n",
       " 'cha': 1653,\n",
       " 'slide': 6026,\n",
       " 'yeah': 7426,\n",
       " 'slow': 6035,\n",
       " 'jamz': 3642,\n",
       " 'toxic': 6762,\n",
       " 'me': 4244,\n",
       " 'stop': 6280,\n",
       " 'more': 4426,\n",
       " 'txt': 6849,\n",
       " 'do': 2287,\n",
       " 'thought': 6626,\n",
       " 'put': 5330,\n",
       " 'it': 3612,\n",
       " 'back': 1149,\n",
       " 'box': 1401,\n",
       " 'hey': 3288,\n",
       " 'will': 7264,\n",
       " 'be': 1220,\n",
       " 'late': 3878,\n",
       " 'at': 1076,\n",
       " 'amk': 909,\n",
       " 'need': 4563,\n",
       " 'drink': 2359,\n",
       " 'tea': 6498,\n",
       " 'coffee': 1815,\n",
       " 'yes': 7437,\n",
       " 'place': 5051,\n",
       " 'town': 6761,\n",
       " 'meet': 4263,\n",
       " 'exciting': 2598,\n",
       " 'adult': 807,\n",
       " 'singles': 5980,\n",
       " 'now': 4670,\n",
       " 'uk': 6875,\n",
       " 'chat': 1680,\n",
       " '86688': 661,\n",
       " 'msg': 4456,\n",
       " 'tired': 6667,\n",
       " 'haven': 3232,\n",
       " 'slept': 6024,\n",
       " 'well': 7204,\n",
       " 'past': 4930,\n",
       " 'few': 2713,\n",
       " 'nights': 4612,\n",
       " 'unni': 6929,\n",
       " 'thank': 6566,\n",
       " 'dear': 2112,\n",
       " 'recharge': 5452,\n",
       " 'rakhesh': 5379,\n",
       " 'business': 1513,\n",
       " 'but': 1516,\n",
       " 'knackered': 3805,\n",
       " 'came': 1567,\n",
       " 'home': 3336,\n",
       " 'went': 7208,\n",
       " 'sleep': 6017,\n",
       " 'good': 3068,\n",
       " 'this': 6615,\n",
       " 'full': 2920,\n",
       " 'time': 6661,\n",
       " 'work': 7341,\n",
       " 'lark': 3874,\n",
       " 'urgent': 6963,\n",
       " 'mobile': 4384,\n",
       " 'number': 4682,\n",
       " 'has': 3223,\n",
       " 'awarded': 1125,\n",
       " '2000': 336,\n",
       " 'guaranteed': 3145,\n",
       " '09058094454': 172,\n",
       " 'from': 2903,\n",
       " 'land': 3856,\n",
       " 'line': 3975,\n",
       " '3030': 416,\n",
       " '12hrs': 277,\n",
       " 'pple': 5167,\n",
       " '700': 588,\n",
       " 'excellent': 2594,\n",
       " 'location': 4018,\n",
       " 'wif': 7255,\n",
       " 'breakfast': 1434,\n",
       " 'hamper': 3188,\n",
       " 'noe': 4629,\n",
       " 'wat': 7151,\n",
       " 'dat': 2092,\n",
       " 'sells': 5810,\n",
       " '4d': 496,\n",
       " 'closes': 1788,\n",
       " 'free': 2870,\n",
       " 'entry': 2516,\n",
       " 'gr8prizes': 3097,\n",
       " 'wkly': 7309,\n",
       " 'comp': 1853,\n",
       " 'chance': 1658,\n",
       " 'win': 7267,\n",
       " 'latest': 3882,\n",
       " 'nokia': 4634,\n",
       " '8800': 673,\n",
       " 'psp': 5298,\n",
       " '250': 359,\n",
       " 'cash': 1615,\n",
       " 'every': 2565,\n",
       " 'wk': 7305,\n",
       " 'great': 3116,\n",
       " '80878': 621,\n",
       " 'http': 3393,\n",
       " 'www': 7388,\n",
       " 'com': 1835,\n",
       " '08715705022': 128,\n",
       " 'should': 5924,\n",
       " 'made': 4142,\n",
       " 'an': 923,\n",
       " 'appointment': 994,\n",
       " 'camera': 1568,\n",
       " 'sipix': 5983,\n",
       " 'digital': 2236,\n",
       " '09061221066': 191,\n",
       " 'fromm': 2904,\n",
       " 'landline': 3858,\n",
       " 'delivery': 2158,\n",
       " 'within': 7298,\n",
       " '28': 366,\n",
       " 'was': 7144,\n",
       " 'she': 5880,\n",
       " 'looking': 4047,\n",
       " 'yup': 7475,\n",
       " 'leh': 3926,\n",
       " 'probably': 5248,\n",
       " 'gotta': 3090,\n",
       " 'check': 1690,\n",
       " 'leo': 3933,\n",
       " 'nope': 4645,\n",
       " 'juz': 3732,\n",
       " 'off': 4717,\n",
       " 'oh': 4736,\n",
       " 'fuck': 2912,\n",
       " 'juswoke': 3731,\n",
       " 'bed': 1237,\n",
       " 'boatin': 1362,\n",
       " 'docks': 2292,\n",
       " 'wid': 7253,\n",
       " '25': 358,\n",
       " 'year': 7427,\n",
       " 'old': 4752,\n",
       " 'spinout': 6182,\n",
       " 'giv': 3026,\n",
       " 'da': 2065,\n",
       " 'gossip': 3084,\n",
       " 'l8r': 3834,\n",
       " 'xxx': 7405,\n",
       " 'usual': 6986,\n",
       " 'iam': 3432,\n",
       " 'fine': 2746,\n",
       " 'happy': 3213,\n",
       " 'amp': 917,\n",
       " 'doing': 2308,\n",
       " 'got': 3085,\n",
       " 'shitload': 5903,\n",
       " 'diamonds': 2215,\n",
       " 'though': 6625,\n",
       " 'cutest': 2057,\n",
       " 'girl': 3021,\n",
       " 'world': 7347,\n",
       " 'gud': 3147,\n",
       " 'mrng': 4454,\n",
       " 'hav': 3228,\n",
       " 'nice': 4603,\n",
       " 'day': 2099,\n",
       " 'did': 2220,\n",
       " 'chechi': 1689,\n",
       " 'drug': 2374,\n",
       " 'anymore': 962,\n",
       " 'if': 3456,\n",
       " 'wasn': 7146,\n",
       " 'paying': 4945,\n",
       " 'attention': 1088,\n",
       " 'morning': 4430,\n",
       " 'my': 4505,\n",
       " 'love': 4074,\n",
       " 'wish': 7287,\n",
       " 'feeling': 2700,\n",
       " 'opportunity': 4793,\n",
       " 'last': 3876,\n",
       " 'babe': 1143,\n",
       " 'kiss': 3799,\n",
       " 'please': 5072,\n",
       " 'haha': 3173,\n",
       " 'awesome': 1127,\n",
       " 'might': 4313,\n",
       " 'doin': 2306,\n",
       " 'tonight': 6725,\n",
       " 'talk': 6466,\n",
       " 'pa': 4867,\n",
       " 'am': 901,\n",
       " 'able': 734,\n",
       " 'dont': 2320,\n",
       " 'can': 1572,\n",
       " 'any': 959,\n",
       " 'major': 4163,\n",
       " 'roles': 5624,\n",
       " 'community': 1852,\n",
       " 'outreach': 4843,\n",
       " 'mel': 4273,\n",
       " 'money': 4411,\n",
       " 'steve': 6265,\n",
       " 'mate': 4216,\n",
       " 'keep': 3762,\n",
       " 'posted': 5151,\n",
       " 'anyways': 972,\n",
       " 'gym': 3165,\n",
       " 'whatever': 7222,\n",
       " 'smiles': 6053,\n",
       " 'having': 3235,\n",
       " 'miss': 4350,\n",
       " 'already': 892,\n",
       " 'get': 2999,\n",
       " 'mystery': 4512,\n",
       " 'solved': 6091,\n",
       " 'opened': 4783,\n",
       " 'email': 2475,\n",
       " 'sent': 5824,\n",
       " 'another': 945,\n",
       " 'batch': 1194,\n",
       " 'isn': 3607,\n",
       " 'sweetie': 6427,\n",
       " 'hello': 3269,\n",
       " 'lover': 4079,\n",
       " 'goes': 3050,\n",
       " 'new': 4590,\n",
       " 'job': 3685,\n",
       " 'think': 6606,\n",
       " 'wake': 7112,\n",
       " 'slave': 6016,\n",
       " 'teasing': 6508,\n",
       " 'across': 770,\n",
       " 'sea': 5773,\n",
       " 'someonone': 6097,\n",
       " 'trying': 6816,\n",
       " 'contact': 1908,\n",
       " 'via': 7032,\n",
       " 'our': 4835,\n",
       " 'dating': 2096,\n",
       " 'service': 5835,\n",
       " 'find': 2743,\n",
       " 'who': 7241,\n",
       " 'could': 1957,\n",
       " '09064015307': 208,\n",
       " 'box334sk38ch': 1407,\n",
       " 'll': 4008,\n",
       " 'rcv': 5405,\n",
       " 'msgs': 4462,\n",
       " 'svc': 6414,\n",
       " 'hardcore': 3215,\n",
       " 'services': 5836,\n",
       " 'text': 6550,\n",
       " '69988': 575,\n",
       " 'nothing': 4662,\n",
       " 'must': 4496,\n",
       " 'age': 833,\n",
       " 'verify': 7025,\n",
       " 'yr': 7467,\n",
       " 'try': 6814,\n",
       " 'again': 830,\n",
       " 'ya': 7412,\n",
       " 'cant': 1583,\n",
       " 'display': 2273,\n",
       " 'internal': 3562,\n",
       " 'subs': 6335,\n",
       " 'extract': 2634,\n",
       " 'them': 6585,\n",
       " 'todays': 6697,\n",
       " 'voda': 7069,\n",
       " 'numbers': 4683,\n",
       " 'ending': 2490,\n",
       " '1225': 272,\n",
       " 'receive': 5445,\n",
       " '50award': 519,\n",
       " 'match': 4214,\n",
       " '08712300220': 98,\n",
       " 'quoting': 5359,\n",
       " '3100': 424,\n",
       " 'standard': 6231,\n",
       " 'rates': 5394,\n",
       " 'app': 983,\n",
       " 'driving': 2366,\n",
       " 'raining': 5370,\n",
       " 'caught': 1628,\n",
       " 'mrt': 4455,\n",
       " 'station': 6248,\n",
       " 'lor': 4053,\n",
       " 'before': 1250,\n",
       " 'midnight': 4311,\n",
       " 'ready': 5420,\n",
       " 'moan': 4381,\n",
       " 'scream': 5761,\n",
       " 'senthil': 5826,\n",
       " 'hsbc': 3391,\n",
       " 'upgrdcentre': 6950,\n",
       " 'orange': 4806,\n",
       " 'may': 4235,\n",
       " 'phone': 5008,\n",
       " 'upgrade': 6948,\n",
       " 'loyalty': 4090,\n",
       " '0207': 9,\n",
       " '153': 299,\n",
       " '9153': 701,\n",
       " 'offer': 4722,\n",
       " 'ends': 2492,\n",
       " '26th': 365,\n",
       " 'july': 3722,\n",
       " 'apply': 991,\n",
       " 'opt': 4797,\n",
       " 'available': 1109,\n",
       " 'ur': 6959,\n",
       " '150': 291,\n",
       " 'worth': 7354,\n",
       " 'discount': 2266,\n",
       " 'vouchers': 7082,\n",
       " 'today': 6696,\n",
       " 'shop': 5911,\n",
       " '85023': 652,\n",
       " 'savamob': 5726,\n",
       " 'offers': 4725,\n",
       " 'cs': 2019,\n",
       " 'pobox84': 5099,\n",
       " 'm263uz': 4125,\n",
       " '00': 0,\n",
       " 'sub': 6330,\n",
       " '16': 302,\n",
       " 'tmr': 6684,\n",
       " 'bugis': 1492,\n",
       " '930': 704,\n",
       " 'captain': 1589,\n",
       " 'vijaykanth': 7046,\n",
       " 'comedy': 1842,\n",
       " 'tv': 6838,\n",
       " 'drunken': 2379,\n",
       " 'grand': 3104,\n",
       " 'prix': 5241,\n",
       " 'later': 3881,\n",
       " '10': 245,\n",
       " 'min': 4323,\n",
       " '09061221061': 190,\n",
       " '28days': 367,\n",
       " 'box177': 1404,\n",
       " 'm221bp': 4122,\n",
       " '2yr': 409,\n",
       " 'warranty': 7142,\n",
       " '150ppm': 297,\n",
       " '99': 711,\n",
       " 'gr8': 3095,\n",
       " 'message': 4295,\n",
       " 'leaving': 3917,\n",
       " 'congrats': 1894,\n",
       " 'school': 5746,\n",
       " 'plans': 5060,\n",
       " 'friday': 2886,\n",
       " 'wait': 7108,\n",
       " 'dunno': 2398,\n",
       " 'wot': 7356,\n",
       " 'hell': 3267,\n",
       " 'im': 3469,\n",
       " 'gonna': 3066,\n",
       " 'weeks': 7194,\n",
       " 'become': 1233,\n",
       " 'slob': 6032,\n",
       " 'bring': 1450,\n",
       " 'some': 6092,\n",
       " 'food': 2816,\n",
       " 'hear': 3248,\n",
       " 'philosophy': 5005,\n",
       " 'say': 5732,\n",
       " 'happen': 3205,\n",
       " 'asked': 1056,\n",
       " 'anna': 935,\n",
       " 'nagar': 4516,\n",
       " 'afternoon': 827,\n",
       " 'round': 5636,\n",
       " 'til': 6659,\n",
       " 'gt': 3142,\n",
       " 'ish': 3603,\n",
       " 'dun': 2396,\n",
       " 'pick': 5022,\n",
       " 'gf': 3009,\n",
       " 'looked': 4045,\n",
       " 'addie': 789,\n",
       " 'monday': 4409,\n",
       " 'sucks': 6351,\n",
       " 'her': 3283,\n",
       " 'hanks': 3201,\n",
       " 'lotsly': 4064,\n",
       " 'pls': 5079,\n",
       " 'play': 5063,\n",
       " 'others': 4829,\n",
       " 'life': 3953,\n",
       " 'sir': 5985,\n",
       " 'waiting': 7111,\n",
       " 'once': 4763,\n",
       " 'depends': 2174,\n",
       " 'individual': 3517,\n",
       " 'hair': 3177,\n",
       " 'dresser': 2358,\n",
       " 'pretty': 5220,\n",
       " 'parents': 4905,\n",
       " 'look': 4044,\n",
       " 'gong': 3065,\n",
       " 'kaypoh': 3758,\n",
       " 'also': 895,\n",
       " 'collecting': 1827,\n",
       " 'coming': 1846,\n",
       " 'saying': 5735,\n",
       " 'order': 4810,\n",
       " 'slippers': 6030,\n",
       " 'cos': 1948,\n",
       " 'had': 3170,\n",
       " 'pay': 4940,\n",
       " 'returning': 5577,\n",
       " 'hungry': 3412,\n",
       " 'gay': 2975,\n",
       " 'guys': 3162,\n",
       " '08718730555': 145,\n",
       " '10p': 255,\n",
       " 'texts': 6560,\n",
       " '08712460324': 110,\n",
       " 'accidentally': 756,\n",
       " 'brought': 1464,\n",
       " 'em': 2474,\n",
       " 'bus': 1509,\n",
       " 'aft': 824,\n",
       " 'lect': 3918,\n",
       " 'lar': 3871,\n",
       " 'car': 1591,\n",
       " 'tot': 6751,\n",
       " 'group': 3134,\n",
       " 'lucky': 4103,\n",
       " 'havent': 3233,\n",
       " 'leave': 3915,\n",
       " 'nobody': 4628,\n",
       " 'names': 4526,\n",
       " 'their': 6583,\n",
       " 'penis': 4963,\n",
       " 'girls': 3024,\n",
       " 'name': 4522,\n",
       " 'story': 6291,\n",
       " 'doesn': 2298,\n",
       " 'add': 785,\n",
       " 'all': 879,\n",
       " 'needs': 4566,\n",
       " 'slowly': 6038,\n",
       " 'vomit': 7075,\n",
       " 'texting': 6557,\n",
       " 'right': 5596,\n",
       " 'ticket': 6649,\n",
       " 'sorry': 6128,\n",
       " 'joined': 3694,\n",
       " 'league': 3910,\n",
       " 'people': 4965,\n",
       " 'touch': 6755,\n",
       " 'mean': 4247,\n",
       " 'times': 6662,\n",
       " 'even': 2558,\n",
       " 'personal': 4985,\n",
       " 'cost': 1950,\n",
       " 'week': 7190,\n",
       " 'open': 4782,\n",
       " 'click': 1778,\n",
       " 'lists': 3996,\n",
       " 'make': 4164,\n",
       " 'list': 3989,\n",
       " 'easy': 2423,\n",
       " 'pie': 5029,\n",
       " 'cool': 1933,\n",
       " 'little': 4000,\n",
       " 'while': 7236,\n",
       " 'getting': 3007,\n",
       " 'soon': 6118,\n",
       " 'oops': 4781,\n",
       " 'thk': 6616,\n",
       " 'haf': 3172,\n",
       " 'enuff': 2519,\n",
       " 'speak': 6159,\n",
       " 'minutes': 4340,\n",
       " 'anyway': 971,\n",
       " 'darren': 2090,\n",
       " 'meeting': 4265,\n",
       " 'ge': 2983,\n",
       " 'den': 2163,\n",
       " 'dinner': 2247,\n",
       " 'xy': 7411,\n",
       " 'feel': 2698,\n",
       " 'awkward': 1128,\n",
       " 'lunch': 4108,\n",
       " 'buying': 1523,\n",
       " 'meh': 4270,\n",
       " 'hi': 3291,\n",
       " 'about': 736,\n",
       " '15pm': 301,\n",
       " 'taunton': 6487,\n",
       " 'church': 1749,\n",
       " 'holla': 3332,\n",
       " 'many': 4187,\n",
       " 'things': 6605,\n",
       " 'its': 3620,\n",
       " 'antibiotic': 958,\n",
       " 'used': 6978,\n",
       " 'chest': 1710,\n",
       " 'abdomen': 728,\n",
       " 'gynae': 3166,\n",
       " 'infections': 3521,\n",
       " 'bone': 1371,\n",
       " 'birthdate': 1315,\n",
       " 'certificate': 1652,\n",
       " 'april': 1004,\n",
       " 'real': 5421,\n",
       " 'date': 2094,\n",
       " 'publish': 5307,\n",
       " 'give': 3027,\n",
       " 'special': 6161,\n",
       " 'treat': 6788,\n",
       " 'secret': 5782,\n",
       " 'way': 7165,\n",
       " 'wishes': 7289,\n",
       " 'cmon': 1800,\n",
       " 'horny': 3361,\n",
       " 'turn': 6834,\n",
       " 'fantasy': 2669,\n",
       " 'hot': 3368,\n",
       " 'sticky': 6267,\n",
       " 'replies': 5530,\n",
       " '50': 515,\n",
       " 'cancel': 1576,\n",
       " 'tel': 6515,\n",
       " 'software': 6085,\n",
       " 'than': 6563,\n",
       " 'bb': 1205,\n",
       " 'wont': 7332,\n",
       " 'use': 6977,\n",
       " 'his': 3305,\n",
       " 'wife': 7256,\n",
       " 'doctor': 2294,\n",
       " 'madam': 4141,\n",
       " 'happened': 3207,\n",
       " 'interview': 3564,\n",
       " 'imma': 3477,\n",
       " 'cause': 1629,\n",
       " 'jay': 3654,\n",
       " 'wants': 7134,\n",
       " 'drugs': 2376,\n",
       " 'rp176781': 5642,\n",
       " 'further': 2934,\n",
       " 'messages': 4297,\n",
       " 'regalportfolio': 5480,\n",
       " 'co': 1805,\n",
       " '08717205546': 130,\n",
       " 'ask': 1054,\n",
       " 'macho': 4136,\n",
       " 'budget': 1487,\n",
       " 'bold': 1367,\n",
       " 'saw': 5731,\n",
       " 'one': 4765,\n",
       " 'dollars': 2313,\n",
       " 'said': 5685,\n",
       " 'mr': 4452,\n",
       " 'foley': 2804,\n",
       " 'won': 7325,\n",
       " 'ipod': 3589,\n",
       " 'prizes': 5245,\n",
       " 'eye': 2635,\n",
       " 'visit': 7060,\n",
       " '82050': 626,\n",
       " 'jesus': 3672,\n",
       " 'christ': 1745,\n",
       " 'bitch': 1319,\n",
       " 'answer': 948,\n",
       " 'fucking': 2915,\n",
       " 'stayin': 6252,\n",
       " 'trouble': 6803,\n",
       " 'stranger': 6296,\n",
       " 'dave': 2097,\n",
       " 'other': 4828,\n",
       " 'sorted': 6130,\n",
       " 'bloke': 1345,\n",
       " 'gona': 3063,\n",
       " 'mum': 4481,\n",
       " 'thinks': 6610,\n",
       " '2getha': 378,\n",
       " 'tessy': 6544,\n",
       " 'favor': 2686,\n",
       " 'convey': 1926,\n",
       " 'birthday': 1316,\n",
       " 'nimya': 4616,\n",
       " 'dnt': 2286,\n",
       " 'forget': 2832,\n",
       " 'shijas': 5892,\n",
       " 'unique': 6917,\n",
       " 'enough': 2508,\n",
       " '30th': 422,\n",
       " 'august': 1096,\n",
       " 'areyouunique': 1018,\n",
       " 'either': 2461,\n",
       " 'works': 7346,\n",
       " 'years': 7428,\n",
       " 'doesnt': 2299,\n",
       " 'bother': 1394,\n",
       " 'would': 7358,\n",
       " 'ip': 3584,\n",
       " 'address': 791,\n",
       " 'test': 6545,\n",
       " 'considering': 1903,\n",
       " 'computer': 1874,\n",
       " 'minecraft': 4329,\n",
       " 'server': 5834,\n",
       " 'thts': 6642,\n",
       " 'god': 3049,\n",
       " 'gift': 3014,\n",
       " 'birds': 1311,\n",
       " 'humans': 3407,\n",
       " 'natural': 4544,\n",
       " 'frm': 2894,\n",
       " 'reverse': 5583,\n",
       " 'cheating': 1688,\n",
       " 'mathematics': 4219,\n",
       " 'marry': 4203,\n",
       " 'lovers': 4081,\n",
       " 'becz': 1236,\n",
       " 'they': 6601,\n",
       " 'undrstndng': 6904,\n",
       " 'avoids': 1120,\n",
       " 'problems': 5251,\n",
       " 'dis': 2259,\n",
       " 'wil': 7261,\n",
       " 'news': 4595,\n",
       " 'by': 1532,\n",
       " 'person': 4984,\n",
       " 'tomorrow': 6717,\n",
       " 'best': 1277,\n",
       " 'break': 1432,\n",
       " 'chain': 1655,\n",
       " 'suffer': 6355,\n",
       " 'frnds': 2897,\n",
       " 'mins': 4336,\n",
       " 'whn': 7240,\n",
       " 'read': 5416,\n",
       " 'difficult': 2232,\n",
       " 'simple': 5971,\n",
       " 'enter': 2510,\n",
       " 'same': 5700,\n",
       " 'elaine': 2466,\n",
       " 'confirmed': 1890,\n",
       " 'onum': 4775,\n",
       " 'ela': 2463,\n",
       " 'normal': 4651,\n",
       " 'two': 6847,\n",
       " 'cartons': 1612,\n",
       " 'very': 7029,\n",
       " 'pleased': 5073,\n",
       " 'shelves': 5886,\n",
       " 'means': 4251,\n",
       " 'february': 2695,\n",
       " 'stay': 6250,\n",
       " 'down': 2340,\n",
       " 'hustle': 3422,\n",
       " 'forth': 2845,\n",
       " 'during': 2402,\n",
       " 'audition': 1093,\n",
       " 'season': 5776,\n",
       " 'since': 5975,\n",
       " 'sister': 5988,\n",
       " 'moved': 4444,\n",
       " 'away': 1126,\n",
       " 'harlem': 3220,\n",
       " 'theory': 6591,\n",
       " 'going': 3055,\n",
       " 'book': 1375,\n",
       " '21': 345,\n",
       " 'coz': 1974,\n",
       " 'wanna': 7128,\n",
       " 'jiayin': 3679,\n",
       " 'isnt': 3608,\n",
       " 'head': 3240,\n",
       " 'usf': 6982,\n",
       " 'fifteen': 2722,\n",
       " 'texted': 6556,\n",
       " 'finished': 2751,\n",
       " 'long': 4040,\n",
       " 'ago': 839,\n",
       " 'showered': 5932,\n",
       " 'er': 2527,\n",
       " 'ything': 7469,\n",
       " 'freemsg': 2875,\n",
       " 'baby': 1146,\n",
       " 'wow': 7362,\n",
       " 'cam': 1565,\n",
       " 'moby': 4393,\n",
       " 'pic': 5021,\n",
       " 'fancy': 2665,\n",
       " 'w8in': 7096,\n",
       " '4utxt': 511,\n",
       " 'rply': 5644,\n",
       " '82242': 628,\n",
       " 'hlp': 3313,\n",
       " '08712317606': 100,\n",
       " 'msg150p': 4457,\n",
       " '2rcv': 400,\n",
       " 'practice': 5175,\n",
       " 'smart': 6043,\n",
       " '200': 335,\n",
       " 'weekly': 7193,\n",
       " 'quiz': 5356,\n",
       " '85222': 654,\n",
       " 'winnersclub': 7278,\n",
       " 'po': 5089,\n",
       " '84': 644,\n",
       " 'm26': 4124,\n",
       " '3uz': 458,\n",
       " 'gbp1': 2979,\n",
       " 'anthony': 956,\n",
       " 'bringing': 1451,\n",
       " 'fees': 2703,\n",
       " 'rent': 5519,\n",
       " 'stuff': 6320,\n",
       " 'thats': 6577,\n",
       " 'help': 3272,\n",
       " 'points': 5107,\n",
       " 'cultures': 2035,\n",
       " 'module': 4397,\n",
       " 'missing': 4354,\n",
       " 'plenty': 5076,\n",
       " 'seem': 5795,\n",
       " 'pub': 5305,\n",
       " 'tone': 6719,\n",
       " 'mob': 4382,\n",
       " 'nok': 4633,\n",
       " '87021': 663,\n",
       " '1st': 324,\n",
       " 'txtin': 6853,\n",
       " 'friends': 2890,\n",
       " 'hl': 3311,\n",
       " '4info': 502,\n",
       " 'died': 2226,\n",
       " 'didn': 2222,\n",
       " 'family': 2661,\n",
       " 'str': 6292,\n",
       " 'orchard': 4809,\n",
       " 'per': 4967,\n",
       " 'request': 5537,\n",
       " 'maangalyam': 4131,\n",
       " 'alaipayuthe': 865,\n",
       " 'set': 5839,\n",
       " 'callertune': 1557,\n",
       " 'callers': 1556,\n",
       " 'press': 5215,\n",
       " 'copy': 1940,\n",
       " '0776xxxxxxx': 31,\n",
       " 'invited': 3578,\n",
       " 'xchat': 7395,\n",
       " 'final': 2739,\n",
       " 'attempt': 1085,\n",
       " 'msgrcvdhg': 4461,\n",
       " 'suite342': 6366,\n",
       " '2lands': 385,\n",
       " 'row': 5639,\n",
       " 'w1j6hl': 7092,\n",
       " 'ldn': 3902,\n",
       " '18yrs': 311,\n",
       " 'audrie': 1095,\n",
       " 'lousy': 4071,\n",
       " 'autocorrect': 1106,\n",
       " 'after': 825,\n",
       " 'quit': 5353,\n",
       " 'lei': 3927,\n",
       " 'shd': 5879,\n",
       " 'sch': 5744,\n",
       " 'hr': 3388,\n",
       " 'oni': 4767,\n",
       " 'ah': 841,\n",
       " 'confuses': 1893,\n",
       " 'maybe': 4237,\n",
       " 'wrong': 7376,\n",
       " 'thing': 6604,\n",
       " 'sort': 6129,\n",
       " 'tho': 6621,\n",
       " 'called': 1554,\n",
       " 'dad': 2068,\n",
       " 'oredi': 4813,\n",
       " 'boy': 1414,\n",
       " 'father': 2679,\n",
       " 'power': 5164,\n",
       " 'frndship': 2898,\n",
       " 'were': 7210,\n",
       " 'otherwise': 4830,\n",
       " 'nalla': 4520,\n",
       " 'adi': 793,\n",
       " 'entey': 2513,\n",
       " 'nattil': 4543,\n",
       " 'kittum': 3802,\n",
       " 'online': 4769,\n",
       " 'yep': 7435,\n",
       " 'house': 3377,\n",
       " 'sunday': 6376,\n",
       " 'studying': 6319,\n",
       " 'next': 4599,\n",
       " 'weekend': 7191,\n",
       " 'cine': 1751,\n",
       " 'plaza': 5070,\n",
       " 'mah': 4151,\n",
       " 'threats': 6631,\n",
       " 'sales': 5691,\n",
       " 'executive': 2603,\n",
       " 'shifad': 5891,\n",
       " 'raised': 5372,\n",
       " 'complaint': 1863,\n",
       " 'against': 831,\n",
       " 'official': 4728,\n",
       " 'str8': 6293,\n",
       " 'each': 2408,\n",
       " '8007': 613,\n",
       " 'classic': 1766,\n",
       " 'hit': 3307,\n",
       " 'polys': 5121,\n",
       " '200p': 341,\n",
       " 'pity': 5046,\n",
       " 'mood': 4423,\n",
       " 'suggestions': 6364,\n",
       " 'space': 6148,\n",
       " 'gives': 3029,\n",
       " 'everything': 2574,\n",
       " 'remember': 5505,\n",
       " 'furniture': 2933,\n",
       " 'yours': 7462,\n",
       " 'around': 1031,\n",
       " 'move': 4443,\n",
       " 'lock': 4021,\n",
       " 'locks': 4022,\n",
       " 'key': 3772,\n",
       " 'jenne': 3666,\n",
       " 'running': 5664,\n",
       " 'admit': 799,\n",
       " 'mad': 4138,\n",
       " 'where': 7231,\n",
       " 'correction': 1945,\n",
       " 'let': 3940,\n",
       " 'lets': 3941,\n",
       " 'run': 5663,\n",
       " 'fighting': 2726,\n",
       " 'lose': 4055,\n",
       " 'bt': 1478,\n",
       " 'fightng': 2727,\n",
       " 'some1': 6093,\n",
       " 'close': 1784,\n",
       " 'dificult': 2234,\n",
       " 'whats': 7223,\n",
       " 'ay': 1134,\n",
       " 'wana': 7127,\n",
       " 'sat': 5717,\n",
       " 'wkg': 7308,\n",
       " 'mmmmm': 4374,\n",
       " 'sooooo': 6123,\n",
       " 'words': 7340,\n",
       " 'mmmm': 4373,\n",
       " 'lion': 3982,\n",
       " 'devouring': 2205,\n",
       " 'mom': 4404,\n",
       " 'ugh': 6869,\n",
       " 'apologize': 981,\n",
       " ...}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00', '000', '000pes', '008704050406', '0089', '0121', '01223585236', '01223585334', '02', '0207', '02072069400', '02073162414', '02085076972', '021', '03', '04', '0430', '05', '050703', '0578', '06', '07', '07008009200', '07046744435', '07090298926', '07099833605', '07123456789', '0721072', '07732584351', '07734396839', '07753741225', '0776xxxxxxx', '07781482378', '07786200117', '077xxx', '07801543489', '07808', '07808247860', '07815296484', '07821230901', '07880867867', '07946746291', '0796xxxxxx', '07973788240', '07xxxxxxxxx', '08', '0800', '08000407165', '08000776320', '08000839402']\n"
     ]
    }
   ],
   "source": [
    "X_train_tokens = vect.get_feature_names()\n",
    "print(X_train_tokens[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['yet', 'yetty', 'yetunde', 'yhl', 'yifeng', 'yijue', 'ym', 'ymca', 'yo', 'yoga', 'yogasana', 'yor', 'yorge', 'you', 'youdoing', 'youi', 'young', 'younger', 'your', 'youre', 'yourinclusive', 'yourjob', 'yours', 'yourself', 'youuuuu', 'yoville', 'yoyyooo', 'yr', 'yrs', 'ything', 'yummy', 'yun', 'yunny', 'yuo', 'yuou', 'yup', 'yupz', 'zac', 'zealand', 'zebra', 'zed', 'zeros', 'zhong', 'zindgi', 'zoe', 'zogtorius', 'zoom', 'zouk', 'zyada', 'èn']\n"
     ]
    }
   ],
   "source": [
    "print(X_train_tokens[-50:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  1., ...,  0.,  1.,  1.],\n",
       "       [ 7., 22.,  0., ...,  1.,  0.,  0.]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# feature count per class\n",
    "nb.feature_count_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of times each token appears across all HAM messages\n",
    "ham_token_count = nb.feature_count_[0, :]\n",
    "\n",
    "# number of times each token appears across all SPAM messages\n",
    "spam_token_count = nb.feature_count_[1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ham</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>token</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000</th>\n",
       "      <td>0.0</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000pes</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>008704050406</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0089</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              ham  spam\n",
       "token                  \n",
       "00            0.0   7.0\n",
       "000           0.0  22.0\n",
       "000pes        1.0   0.0\n",
       "008704050406  0.0   2.0\n",
       "0089          0.0   1.0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a table of tokens with their separate ham and spam counts\n",
    "tokens = pd.DataFrame({'token':X_train_tokens, 'ham':ham_token_count, 'spam':spam_token_count}).set_index('token')\n",
    "tokens.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ham</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>token</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>toughest</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fucking</th>\n",
       "      <td>14.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>baaaaabe</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reckon</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cochin</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ham  spam\n",
       "token               \n",
       "toughest   2.0   0.0\n",
       "fucking   14.0   0.0\n",
       "baaaaabe   1.0   0.0\n",
       "reckon     1.0   0.0\n",
       "cochin     2.0   0.0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens.sample(5, random_state=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes counts the number of observations in each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3618.,  561.])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb.class_count_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add 1 to ham and spam counts to avoid dividing by 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ham</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>token</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>toughest</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fucking</th>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>baaaaabe</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reckon</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cochin</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ham  spam\n",
       "token               \n",
       "toughest   3.0   1.0\n",
       "fucking   15.0   1.0\n",
       "baaaaabe   2.0   1.0\n",
       "reckon     2.0   1.0\n",
       "cochin     3.0   1.0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens['ham'] = tokens.ham + 1\n",
    "tokens['spam'] = tokens.spam + 1\n",
    "tokens.sample(5, random_state=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ham</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>token</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>toughest</th>\n",
       "      <td>0.000829</td>\n",
       "      <td>0.001783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fucking</th>\n",
       "      <td>0.004146</td>\n",
       "      <td>0.001783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>baaaaabe</th>\n",
       "      <td>0.000553</td>\n",
       "      <td>0.001783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reckon</th>\n",
       "      <td>0.000553</td>\n",
       "      <td>0.001783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cochin</th>\n",
       "      <td>0.000829</td>\n",
       "      <td>0.001783</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               ham      spam\n",
       "token                       \n",
       "toughest  0.000829  0.001783\n",
       "fucking   0.004146  0.001783\n",
       "baaaaabe  0.000553  0.001783\n",
       "reckon    0.000553  0.001783\n",
       "cochin    0.000829  0.001783"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert the ham and spam counts into frequencies\n",
    "tokens['ham'] = tokens.ham / nb.class_count_[0]\n",
    "tokens['spam'] = tokens.spam / nb.class_count_[1]\n",
    "tokens.sample(5, random_state=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the ratio of spam-to-ham for each token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ham</th>\n",
       "      <th>spam</th>\n",
       "      <th>spam_ratio</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>token</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>toughest</th>\n",
       "      <td>0.000829</td>\n",
       "      <td>0.001783</td>\n",
       "      <td>2.149733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fucking</th>\n",
       "      <td>0.004146</td>\n",
       "      <td>0.001783</td>\n",
       "      <td>0.429947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>baaaaabe</th>\n",
       "      <td>0.000553</td>\n",
       "      <td>0.001783</td>\n",
       "      <td>3.224599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reckon</th>\n",
       "      <td>0.000553</td>\n",
       "      <td>0.001783</td>\n",
       "      <td>3.224599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cochin</th>\n",
       "      <td>0.000829</td>\n",
       "      <td>0.001783</td>\n",
       "      <td>2.149733</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               ham      spam  spam_ratio\n",
       "token                                   \n",
       "toughest  0.000829  0.001783    2.149733\n",
       "fucking   0.004146  0.001783    0.429947\n",
       "baaaaabe  0.000553  0.001783    3.224599\n",
       "reckon    0.000553  0.001783    3.224599\n",
       "cochin    0.000829  0.001783    2.149733"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens['spam_ratio'] = tokens.spam / tokens.ham\n",
    "tokens.sample(5, random_state=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the DataFrame sorted by spam_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ham</th>\n",
       "      <th>spam</th>\n",
       "      <th>spam_ratio</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>token</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>claim</th>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.140820</td>\n",
       "      <td>509.486631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prize</th>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.105169</td>\n",
       "      <td>380.502674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150p</th>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.096257</td>\n",
       "      <td>348.256684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tone</th>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.083779</td>\n",
       "      <td>303.112299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.071301</td>\n",
       "      <td>257.967914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>guaranteed</th>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.067736</td>\n",
       "      <td>245.069519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cs</th>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.062389</td>\n",
       "      <td>225.721925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.060606</td>\n",
       "      <td>219.272727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.060606</td>\n",
       "      <td>219.272727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.057041</td>\n",
       "      <td>206.374332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>awarded</th>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.055258</td>\n",
       "      <td>199.925134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>uk</th>\n",
       "      <td>0.000553</td>\n",
       "      <td>0.099822</td>\n",
       "      <td>180.577540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ringtone</th>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.048128</td>\n",
       "      <td>174.128342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>www</th>\n",
       "      <td>0.000829</td>\n",
       "      <td>0.142602</td>\n",
       "      <td>171.978610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rate</th>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.044563</td>\n",
       "      <td>161.229947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tones</th>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.044563</td>\n",
       "      <td>161.229947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150ppm</th>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.044563</td>\n",
       "      <td>161.229947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000</th>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.040998</td>\n",
       "      <td>148.331551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weekly</th>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.039216</td>\n",
       "      <td>141.882353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>entry</th>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.039216</td>\n",
       "      <td>141.882353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mob</th>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.035651</td>\n",
       "      <td>128.983957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.000553</td>\n",
       "      <td>0.069519</td>\n",
       "      <td>125.759358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8007</th>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.033868</td>\n",
       "      <td>122.534759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>collection</th>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.033868</td>\n",
       "      <td>122.534759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10p</th>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.033868</td>\n",
       "      <td>122.534759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>valid</th>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.033868</td>\n",
       "      <td>122.534759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>poly</th>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>109.636364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800</th>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>109.636364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750</th>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>109.636364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5000</th>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>109.636364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>where</th>\n",
       "      <td>0.025428</td>\n",
       "      <td>0.003565</td>\n",
       "      <td>0.140200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>but</th>\n",
       "      <td>0.092040</td>\n",
       "      <td>0.012478</td>\n",
       "      <td>0.135569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>did</th>\n",
       "      <td>0.026534</td>\n",
       "      <td>0.003565</td>\n",
       "      <td>0.134358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>went</th>\n",
       "      <td>0.013543</td>\n",
       "      <td>0.001783</td>\n",
       "      <td>0.131616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ll</th>\n",
       "      <td>0.055832</td>\n",
       "      <td>0.007130</td>\n",
       "      <td>0.127707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wait</th>\n",
       "      <td>0.014096</td>\n",
       "      <td>0.001783</td>\n",
       "      <td>0.126455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lol</th>\n",
       "      <td>0.014096</td>\n",
       "      <td>0.001783</td>\n",
       "      <td>0.126455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>my</th>\n",
       "      <td>0.155611</td>\n",
       "      <td>0.019608</td>\n",
       "      <td>0.126006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feel</th>\n",
       "      <td>0.014373</td>\n",
       "      <td>0.001783</td>\n",
       "      <td>0.124023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nice</th>\n",
       "      <td>0.014373</td>\n",
       "      <td>0.001783</td>\n",
       "      <td>0.124023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anything</th>\n",
       "      <td>0.014925</td>\n",
       "      <td>0.001783</td>\n",
       "      <td>0.119430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sure</th>\n",
       "      <td>0.014925</td>\n",
       "      <td>0.001783</td>\n",
       "      <td>0.119430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>something</th>\n",
       "      <td>0.014925</td>\n",
       "      <td>0.001783</td>\n",
       "      <td>0.119430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cos</th>\n",
       "      <td>0.014925</td>\n",
       "      <td>0.001783</td>\n",
       "      <td>0.119430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>come</th>\n",
       "      <td>0.047264</td>\n",
       "      <td>0.005348</td>\n",
       "      <td>0.113144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>morning</th>\n",
       "      <td>0.016307</td>\n",
       "      <td>0.001783</td>\n",
       "      <td>0.109308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sorry</th>\n",
       "      <td>0.032891</td>\n",
       "      <td>0.003565</td>\n",
       "      <td>0.108390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ask</th>\n",
       "      <td>0.017413</td>\n",
       "      <td>0.001783</td>\n",
       "      <td>0.102368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>already</th>\n",
       "      <td>0.017689</td>\n",
       "      <td>0.001783</td>\n",
       "      <td>0.100769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>said</th>\n",
       "      <td>0.017966</td>\n",
       "      <td>0.001783</td>\n",
       "      <td>0.099218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amp</th>\n",
       "      <td>0.019071</td>\n",
       "      <td>0.001783</td>\n",
       "      <td>0.093467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doing</th>\n",
       "      <td>0.019900</td>\n",
       "      <td>0.001783</td>\n",
       "      <td>0.089572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oh</th>\n",
       "      <td>0.023217</td>\n",
       "      <td>0.001783</td>\n",
       "      <td>0.076776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>later</th>\n",
       "      <td>0.030956</td>\n",
       "      <td>0.001783</td>\n",
       "      <td>0.057582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lor</th>\n",
       "      <td>0.031509</td>\n",
       "      <td>0.001783</td>\n",
       "      <td>0.056572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>da</th>\n",
       "      <td>0.032338</td>\n",
       "      <td>0.001783</td>\n",
       "      <td>0.055121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>she</th>\n",
       "      <td>0.037313</td>\n",
       "      <td>0.001783</td>\n",
       "      <td>0.047772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>he</th>\n",
       "      <td>0.045329</td>\n",
       "      <td>0.001783</td>\n",
       "      <td>0.039324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lt</th>\n",
       "      <td>0.065782</td>\n",
       "      <td>0.001783</td>\n",
       "      <td>0.027097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gt</th>\n",
       "      <td>0.066059</td>\n",
       "      <td>0.001783</td>\n",
       "      <td>0.026984</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7490 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 ham      spam  spam_ratio\n",
       "token                                     \n",
       "claim       0.000276  0.140820  509.486631\n",
       "prize       0.000276  0.105169  380.502674\n",
       "150p        0.000276  0.096257  348.256684\n",
       "tone        0.000276  0.083779  303.112299\n",
       "18          0.000276  0.071301  257.967914\n",
       "guaranteed  0.000276  0.067736  245.069519\n",
       "cs          0.000276  0.062389  225.721925\n",
       "500         0.000276  0.060606  219.272727\n",
       "1000        0.000276  0.060606  219.272727\n",
       "100         0.000276  0.057041  206.374332\n",
       "awarded     0.000276  0.055258  199.925134\n",
       "uk          0.000553  0.099822  180.577540\n",
       "ringtone    0.000276  0.048128  174.128342\n",
       "www         0.000829  0.142602  171.978610\n",
       "rate        0.000276  0.044563  161.229947\n",
       "tones       0.000276  0.044563  161.229947\n",
       "150ppm      0.000276  0.044563  161.229947\n",
       "000         0.000276  0.040998  148.331551\n",
       "weekly      0.000276  0.039216  141.882353\n",
       "entry       0.000276  0.039216  141.882353\n",
       "mob         0.000276  0.035651  128.983957\n",
       "16          0.000553  0.069519  125.759358\n",
       "8007        0.000276  0.033868  122.534759\n",
       "collection  0.000276  0.033868  122.534759\n",
       "10p         0.000276  0.033868  122.534759\n",
       "valid       0.000276  0.033868  122.534759\n",
       "poly        0.000276  0.030303  109.636364\n",
       "800         0.000276  0.030303  109.636364\n",
       "750         0.000276  0.030303  109.636364\n",
       "5000        0.000276  0.030303  109.636364\n",
       "...              ...       ...         ...\n",
       "where       0.025428  0.003565    0.140200\n",
       "but         0.092040  0.012478    0.135569\n",
       "did         0.026534  0.003565    0.134358\n",
       "went        0.013543  0.001783    0.131616\n",
       "ll          0.055832  0.007130    0.127707\n",
       "wait        0.014096  0.001783    0.126455\n",
       "lol         0.014096  0.001783    0.126455\n",
       "my          0.155611  0.019608    0.126006\n",
       "feel        0.014373  0.001783    0.124023\n",
       "nice        0.014373  0.001783    0.124023\n",
       "anything    0.014925  0.001783    0.119430\n",
       "sure        0.014925  0.001783    0.119430\n",
       "something   0.014925  0.001783    0.119430\n",
       "cos         0.014925  0.001783    0.119430\n",
       "come        0.047264  0.005348    0.113144\n",
       "morning     0.016307  0.001783    0.109308\n",
       "sorry       0.032891  0.003565    0.108390\n",
       "ask         0.017413  0.001783    0.102368\n",
       "already     0.017689  0.001783    0.100769\n",
       "said        0.017966  0.001783    0.099218\n",
       "amp         0.019071  0.001783    0.093467\n",
       "doing       0.019900  0.001783    0.089572\n",
       "oh          0.023217  0.001783    0.076776\n",
       "later       0.030956  0.001783    0.057582\n",
       "lor         0.031509  0.001783    0.056572\n",
       "da          0.032338  0.001783    0.055121\n",
       "she         0.037313  0.001783    0.047772\n",
       "he          0.045329  0.001783    0.039324\n",
       "lt          0.065782  0.001783    0.027097\n",
       "gt          0.066059  0.001783    0.026984\n",
       "\n",
       "[7490 rows x 3 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens.sort_values('spam_ratio', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51.593582887700535"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens.loc['00', 'spam_ratio']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning the vectorizer\n",
    "Do you see any potential to enhance the vectorizer? Think about the following questions:  \n",
    "__Are all word equally important?__  \n",
    "__Do you think there are \"noise words\" which negatively influence the results?__  \n",
    "__How can we account for the order of words?__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stopwords\n",
    "Stopwords are the most common words in a language. Examples are 'is', 'which' and 'the'. Usually is beneficial to exclude these words in text processing tasks.  \n",
    "The `CountVectorizer` has a `stop_words` parameter:\n",
    "- **stop_words:** string {'english'}, list, or None (default)\n",
    "    - If 'english', a built-in stop word list for English is used.\n",
    "    - If a list, that list is assumed to contain stop words, all of which will be removed from the resulting tokens.\n",
    "    - If None, no stop words will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(stop_words='english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### n-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n-grams concatenate n words to form a token. The following accounts for 1- and 2-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(ngram_range=(1, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Document frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often it's beneficial to exclude words that appear in the majority or just a couple of documents. This is, very frequent or infrequent words. This can be achieved by using the `max_df` and `min_df` parameters of the vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore terms that appear in more than 50% of the documents\n",
    "vect = CountVectorizer(max_df=0.5)\n",
    "\n",
    "# only keep terms that appear in at least 2 documents\n",
    "vect = CountVectorizer(min_df=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A note on Stemming\n",
    "* 'went' and 'go'  \n",
    "* 'kids' and 'kid'  \n",
    "* 'negative' and 'negatively'\n",
    "\n",
    "__What is the pattern?__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process of reducing a word to it's word stem, base or root form is called _stemming_. Scikit-Learn has no powerfull stemmer, but other libraries like the [NLTK](http://www.nltk.org/) have. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tf-idf\n",
    "* Tf-idf can be understood as a modification of the *raw term frequencies* (tf)\n",
    "* The concept behind tf-idf is to downweight terms proportionally to the number of documents in which they occur.\n",
    "* The idea is that terms that occur in many different documents are likely unimportant or don't contain any useful information for Natural Language Processing tasks such as document classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Explanation by example\n",
    "Let consider a dataset containing 3 documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "docs = np.array([\n",
    "        'The sun is shining',\n",
    "        'The weather is sweet',\n",
    "        'The sun is shining and the weather is sweet'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will compute the _term frequency_ (alternatively: Bag-of-Words) $tf(t, d)$. $t$ is the number of times a term occures in a document $d$. Using Scikit-Learn we can quickly get those numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 1, 0, 1, 0],\n",
       "       [0, 1, 0, 0, 1, 1, 1],\n",
       "       [1, 2, 1, 1, 1, 2, 1]], dtype=int64)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer()\n",
    "tf = cv.fit_transform(docs).toarray()\n",
    "tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 5, 'sun': 3, 'is': 1, 'shining': 2, 'weather': 6, 'sweet': 4, 'and': 0}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Secondly, we introduce *inverse document frequency* ($idf$) by defining the term *document frequency* $\\text{df}(d,t)$, which is simply the number of documents $d$ that contain the term $t$. We can then define the idf as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{idf}(t) = log{\\frac{n_d}{1+\\text{df}(d,t)}},$$ \n",
    "where  \n",
    "$n_d$: The total number of documents  \n",
    "$\\text{df}(d,t)$: The number of documents that contain term $t$.\n",
    "\n",
    "Note that the constant 1 is added to the denominator to avoid a zero-division error if a term is not contained in any document in the test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, Let us calculate the idfs of the words \"and\", \"is,\" and \"shining:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idf \"and\": 0.4054651081081644\n",
      "idf \"is\": -0.2876820724517809\n",
      "idf \"shining\": 0.0\n"
     ]
    }
   ],
   "source": [
    "n_docs = len(docs)\n",
    "\n",
    "df_and = 1\n",
    "idf_and = np.log(n_docs / (1 + df_and))\n",
    "print('idf \"and\": %s' % idf_and)\n",
    "\n",
    "df_is = 3\n",
    "idf_is = np.log(n_docs / (1 + df_is))\n",
    "print('idf \"is\": %s' % idf_is)\n",
    "\n",
    "df_shining = 2\n",
    "idf_shining = np.log(n_docs / (1 + df_shining))\n",
    "print('idf \"shining\": %s' % idf_shining)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using those idfs, we can eventually calculate the tf-idfs for the 3rd document:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{tf-idf}(t, d) = \\text{tf}(t, d) \\times \\text{idf}(t),$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tf-idfs in document 3:\n",
      "\n",
      "tf-idf \"and\": 0.4054651081081644\n",
      "tf-idf \"is\": -0.5753641449035618\n",
      "tf-idf \"shining\": 0.0\n"
     ]
    }
   ],
   "source": [
    "print('Tf-idfs in document 3:\\n')\n",
    "print('tf-idf \"and\": %s' % (1 * idf_and))\n",
    "print('tf-idf \"is\": %s' % (2 * idf_is))\n",
    "print('tf-idf \"shining\": %s' % (1 * idf_shining))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tf-idf in Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.09861229, 2.        , 1.40546511])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf = TfidfTransformer(smooth_idf=False, norm=None)\n",
    "tfidf.fit_transform(tf).toarray()[-1][:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Wait! Those numbers aren't the same!__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tf-idf in Scikit-Learn is calculated a little bit differently. Here, the `+1` count is added to the idf, whereas instead of the denominator if the df:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{idf}(t) = log{\\frac{n_d}{\\text{df}(d,t)}} + 1$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.09861228866811"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_and = 1\n",
    "df_and = 1 \n",
    "tf_and * (np.log(n_docs / df_and) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_is = 2\n",
    "df_is = 3 \n",
    "tf_is * (np.log(n_docs / df_is) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4054651081081644"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_shining = 1\n",
    "df_shining = 2 \n",
    "tf_shining * (np.log(n_docs / df_shining) + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization\n",
    "\n",
    "By default, Scikit-Learn performs a normalization. The most common way to normalize the raw term frequency is l2-normalization, i.e., dividing the raw term frequency vector $v$ by its length $||v||_2$ (L2- or Euclidean norm).\n",
    "\n",
    "$$v_{norm} = \\frac{v}{||v||_2} = \\frac{v}{\\sqrt{v{_1}^2 + v{_2}^2 + \\dots + v{_n}^2}}$$\n",
    "\n",
    "__Why is that useful?__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, we would normalize our 3rd document `'The sun is shining and the weather is sweet'` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.46572049, 0.44383662, 0.31189844])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfTransformer(use_idf=True, smooth_idf=False, norm='l2')\n",
    "tfidf.fit_transform(tf).toarray()[-1][:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smooth idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are not quite there. Sckit-Learn also applies smoothing, which changes the original formula as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{idf}(t) = log{\\frac{1 + n_d}{1+\\text{df}(d,t)}} + 1$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.40474829, 0.47810172, 0.30782151])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfTransformer(use_idf=True, smooth_idf=True, norm='l2')\n",
    "tfidf.fit_transform(tf).toarray()[-1][:3]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
