{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Machine Learning #\n",
    "Author: Christin Seifert, licensed under the Creative Commons Attribution 3.0 Unported License https://creativecommons.org/licenses/by/3.0/\n",
    "\n",
    "This is a tutorial for implementing a simple machine learning pipeline aimed at machine learning beginners. \n",
    "In this notebook we will\n",
    "* train classifiers to differentiate types of flowers (we use a very famous and old dataset)\n",
    "* evaluate how well the classifiers do that in general\n",
    "* dig a little deeper on where they might have problems\n",
    "\n",
    "It is assumed that you have some general knowledge on \n",
    "* what a decision tree is and how it works\n",
    "* how the Naive Bayes classifier works\n",
    "* training and testing splits of data sets\n",
    "* evaluation measures for classification (namely accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "First, we import all the python libraries we will need later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pythons scientific computing package and a random number generator\n",
    "import numpy as np\n",
    "import random\n",
    "# here we find some standard machine learning datasets\n",
    "from sklearn import datasets\n",
    "\n",
    "# machine learning classifiers and metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# plotting tool\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, let's get the data, then and have a look at some examples. It seems that there is a lot of variation for some numbers there. Can you make a decision which number you see wih high confidence for each of the examples?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _iris_dataset:\n",
      "\n",
      "Iris plants dataset\n",
      "--------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 150 (50 in each of three classes)\n",
      "    :Number of Attributes: 4 numeric, predictive attributes and the class\n",
      "    :Attribute Information:\n",
      "        - sepal length in cm\n",
      "        - sepal width in cm\n",
      "        - petal length in cm\n",
      "        - petal width in cm\n",
      "        - class:\n",
      "                - Iris-Setosa\n",
      "                - Iris-Versicolour\n",
      "                - Iris-Virginica\n",
      "                \n",
      "    :Summary Statistics:\n",
      "\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "                    Min  Max   Mean    SD   Class Correlation\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "    sepal length:   4.3  7.9   5.84   0.83    0.7826\n",
      "    sepal width:    2.0  4.4   3.05   0.43   -0.4194\n",
      "    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n",
      "    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "    :Class Distribution: 33.3% for each of 3 classes.\n",
      "    :Creator: R.A. Fisher\n",
      "    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
      "    :Date: July, 1988\n",
      "\n",
      "The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\n",
      "from Fisher's paper. Note that it's the same as in R, but not as in the UCI\n",
      "Machine Learning Repository, which has two wrong data points.\n",
      "\n",
      "This is perhaps the best known database to be found in the\n",
      "pattern recognition literature.  Fisher's paper is a classic in the field and\n",
      "is referenced frequently to this day.  (See Duda & Hart, for example.)  The\n",
      "data set contains 3 classes of 50 instances each, where each class refers to a\n",
      "type of iris plant.  One class is linearly separable from the other 2; the\n",
      "latter are NOT linearly separable from each other.\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\n",
      "     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n",
      "     Mathematical Statistics\" (John Wiley, NY, 1950).\n",
      "   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\n",
      "     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n",
      "   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n",
      "     Structure and Classification Rule for Recognition in Partially Exposed\n",
      "     Environments\".  IEEE Transactions on Pattern Analysis and Machine\n",
      "     Intelligence, Vol. PAMI-2, No. 1, 67-71.\n",
      "   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n",
      "     on Information Theory, May 1972, 431-433.\n",
      "   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n",
      "     conceptual clustering system finds 3 classes in the data.\n",
      "   - Many, many more ...\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "data = datasets.load_iris()\n",
    "\n",
    "# and get some description of the data set\n",
    "print(data.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect the data\n",
    "Now we have an overview what is in the data set. The next step is to actually look at the data. Let's look at 5 random items in the data set.\n",
    "Variation to try for yourself:\n",
    "* run the cell multiple times\n",
    "* show 10 instead of 5 instances\n",
    "* show the first 5 instances in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: array([6.2, 2.9, 4.3, 1.3])--> Target class: 1\n",
      "Features: array([7.6, 3. , 6.6, 2.1])--> Target class: 2\n",
      "Features: array([6.4, 3.1, 5.5, 1.8])--> Target class: 2\n",
      "Features: array([6.5, 3. , 5.5, 1.8])--> Target class: 2\n",
      "Features: array([6. , 2.2, 5. , 1.5])--> Target class: 2\n",
      "The classes are called  array(['setosa', 'versicolor', 'virginica'], dtype='<U10')\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,5):\n",
    "    # get the next random number\n",
    "    idx = random.randint(0,len(data.data))\n",
    "    print(\"Features: \" + repr(data.data[idx]) + \"--> Target class: \" + repr(data.target[idx]))\n",
    "    \n",
    "print(\"The classes are called  \" + repr(data.target_names))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train / Test splits\n",
    "We now have established that the dataset contains flowers represented by 4 features each (i.e., the length and the width of the petal and the length and the width of the sepal). The documentation of the data set can be found in the python sklearn documentation: https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html.\n",
    "Since we want to train and evaluate a classifier, we need to set some data aside for evaluation and train on another portion of the data. We create a train and a test set. This step is important, since it ensures that the classifier really learns general patterns and not only just remembers what it has seen before and replicates that from memory. We also want to split the data randomly, to ensure that i) all classes are in the train and in the test set and ii) we do not have the easy/difficulty examples just in one set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split randomly train/test 50%/50%, random seed fixed to replicate results\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.data,data.target,shuffle=True,test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have our training data in `X_train` and the classes in `y_train`, similarly for `_test`. Let's check the sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(75, 4)\n",
      "(75, 4)\n",
      "First example in the training data set\n",
      "  features: array([6.2, 3.4, 5.4, 2.3])\n",
      "  class: 2\n",
      "  class name: 'virginica'\n"
     ]
    }
   ],
   "source": [
    "# investigate the size of the feature matrices\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "# inspect one example\n",
    "print(\"First example in the training data set\")\n",
    "print(\"  features: \" + repr(X_train[1]))\n",
    "print(\"  class: \" + repr(y_train[1]))\n",
    "print(\"  class name: \" + repr(data.target_names[y_train[1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Naive Bayes Classifier\n",
    "Now we are nearly ready to train our first classifier. One thing still needs to be said. Classification is a supervised machine learning task. This means, we give the classifier a feature vector together with the desired output (the target). The targets were also loaded from the original data set and reside in the vector $y_{train}$. Putting things together, the classifier gets a matrix, which contains one row for each image and as many columns as we have features. And it also gets a vector of targets, that is as long as we have images. Thus the number of rows in $X_{train}$ is equal to the length of $y_{train}$. Isn't this neat?\n",
    "\n",
    "Now we finally can train our first model (a model is a trained classifer). The scikit learn library in python uses standard interfaces to all classifiers. This means, no matter which classifier you want to use, the functions you have to call are always named the same (but they might have different parameters). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize the model with standard parameters\n",
    "clf_nb = MultinomialNB()\n",
    "# train the model\n",
    "clf_nb.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Naive Bayes classifier\n",
    "Ok, nice. We have trained a model. In the code, the model is called `clf_nb`. But, is it a good model? To answer this, we need to evaluate the model on data it has not yet seen, that is on `X_test` and the respective labels `y_test`. \n",
    "\n",
    "We do this in two steps:\n",
    "\n",
    "1. We ask the classifier about its opinion by only giving it the test data (without the labels). This step is called **prediction**. We store the results in a vector `y_test_pred_nb`. \n",
    "\n",
    "2. We count how often the classifier's predictions are the same as the correct labels. This step is called **evaluaton**. The counting is already conveniently implemented in the library, so we only need to call a function `accuracy_score()` which returns us the ratio of correct predictions and total items. If you multiply this ratio by 100 you get a value that can be interpreted as \"the classifier is ... percent correct on the test data\".\n",
    "\n",
    "Thus, we can conclude, the classifier has an accuracy of approximately 85%. Or in other words, it misclassifies 15% of the examples. Is this good or bad? Has it learned something? What if we got a value of 50%. Would this be good? \n",
    "\n",
    "Whether it has learned _something_ can be answered quite easily. We could simply compared it to random guessing. There are 10 classes in the data set (digits from 0 to 9). In the test set, there is an equal amount of examples for each class. Or in other words, the examples are uniformly distributed over the classes. You could easily check this by inspecting `y_test`. If the classifier would randomly guess, which digit it sees, it would have a 10% chance of getting it right. So, it has learned quite a lot already by only looking at the pixel values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9733333333333334\n"
     ]
    }
   ],
   "source": [
    "# make predictions with the NB classifier\n",
    "y_test_pred_nb = clf_nb.predict(X_test);\n",
    "a_nb = accuracy_score(y_test, y_test_pred_nb);\n",
    "print(a_nb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the decision tree classifier\n",
    "We train the decision tree classifier in a similar manner as we trained the Naive Bayes classifier. Note, that the function calls are equivalent.\n",
    "\n",
    "_Note: you might notice, that training of a decision tree can take some seconds._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_dt = DecisionTreeClassifier();\n",
    "clf_dt.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the decision tree classifier\n",
    "Now, let's see how well this model performs on the test set. \n",
    "\n",
    "It achieves an accuracy of about 93%, thus getting only 7% of the examples wrong. This seems a bit worse than the Naive Bayes classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9333333333333333\n"
     ]
    }
   ],
   "source": [
    "# make predictions with the decision tree classifier\n",
    "y_test_pred_dt = clf_dt.predict(X_test)\n",
    "a_dt = accuracy_score(y_test, y_test_pred_dt)\n",
    "print(a_dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More detailed error analysis\n",
    "Can we find out more about the mistakes both models still make? If we could, we could probably find ways to improve it. Or it might also be the case that we might find errors in the underlying data (e.g. mislabeled images, images that do not contain digits at all). The latter case is in this example rather unlikely, since this data set has been studied already for a long time and by many different researchers and practicioners. \n",
    "\n",
    "### Confusion matrices\n",
    "One thing we could ask is which digits get often confused with one another. Or more generally, which classes often get confused? We can easily asses this, since we have the predictions and the true labels. So, for each digit we just have to count how often label $l$ in the ground truth is predicted as label $k$. We display this in matrix form, this matrix is called _class confusion matrix_ $C$. Entry $(i,j)$ in this matrix holds the count of how often the target $i$ was predicted as $j$. \n",
    "\n",
    "The strength of the confusion (i.e., the total number of misclassified examples) is indicated with a color in the respective cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get the confusion matrices for both classifiers\n",
    "cm_nb = confusion_matrix(y_test, y_test_pred_nb);\n",
    "cm_dt = confusion_matrix(y_test, y_test_pred_dt);\n",
    "\n",
    "# plot the confusion matrices nicely\n",
    "plt.subplot(1, 2, 1)    \n",
    "plt.title('Decision Tree', fontsize=16)\n",
    "plt.imshow(cm_dt, interpolation='nearest',cmap=plt.cm.binary);\n",
    "plt.tight_layout();\n",
    "plt.colorbar();\n",
    "plt.ylabel('True label');\n",
    "plt.xlabel('Predicted label');\n",
    "plt.xticks(np.arange(3));\n",
    "plt.yticks(np.arange(3));\n",
    "\n",
    "plt.subplot(1, 2, 2) \n",
    "plt.title('Naive Bayes', fontsize=16)\n",
    "plt.imshow(cm_nb, interpolation='nearest',cmap=plt.cm.binary);\n",
    "plt.tight_layout();\n",
    "plt.colorbar();\n",
    "plt.ylabel('True label');\n",
    "plt.xlabel('Predicted label');\n",
    "plt.xticks(np.arange(3));\n",
    "plt.yticks(np.arange(3));\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both plots show a nice dark diagonal. This indicates that most of the examples are predicted correctly (as we know from the accuracy measures). The **decision tree** seems to make its by confusing class 2 with 1 and 1 with 2. If you squint a bit, you can see light gray boxes on the off-diagonal. (The visualization could be improved by removing all the diagonal elements and just focus on the errors, the off-diagonal elements). The **Naive Bayes** classifier on the other hand only predicts class 2 as class 1 wrongly, but not the other way around."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "We have seen a very basic machine learning pipeline. We trained two different classifiers on the same data set and compared them. There are, of course as always ;-) many more things one could try, some of them are:\n",
    "\n",
    "* have a detailed look a the images that were misclassified\n",
    "* use more sophisticated features than just pixel values\n",
    "* try different classifiers\n",
    "* distort the test images and see how robust the classifiers are to noisy input.\n",
    "\n",
    "### Take-Aways\n",
    "1. A classifier needs feature vectors as input, so the input data has first to be transformed in a suitable format.\n",
    "1. Always train and evaluate on different data set splits.\n",
    "1. Accuracy alone is not a helpful measure, you need to compare it to something, at least the random baseline.\n",
    "1. Confusion matrices are a handy tool to pin-point the errors.\n",
    "\n",
    "That's all for today. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
